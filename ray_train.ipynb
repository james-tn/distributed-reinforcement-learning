{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ray Cluster in AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws01ent | westus2 | azureml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = 'rl_talk_pong'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found head compute target. just use it head-gpu-v2\n"
     ]
    }
   ],
   "source": [
    "vnet_name = 'amlvnet'\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# Choose a name for the Ray head cluster\n",
    "# head_compute_name = 'head-gpu-v3'\n",
    "head_compute_name = 'head-gpu-v2'\n",
    "\n",
    "head_compute_min_nodes = 0\n",
    "head_compute_max_nodes = 2\n",
    "\n",
    "# This example uses GPU VM. For using CPU VM, set SKU to STANDARD_D2_V2\n",
    "head_vm_size = 'STANDARD_NC6S_V2'\n",
    "\n",
    "if head_compute_name in ws.compute_targets:\n",
    "    head_compute_target = ws.compute_targets[head_compute_name]\n",
    "    if head_compute_target and type(head_compute_target) is AmlCompute:\n",
    "        if head_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found head compute target. just use it', head_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found head compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new head compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=head_vm_size,\n",
    "        min_nodes=head_compute_min_nodes, \n",
    "        max_nodes=head_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the cluster\n",
    "    head_compute_target = ComputeTarget.create(ws, head_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    head_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(head_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your Ray worker compute target\n",
    "worker_compute_name = 'worker-cpu-f32'\n",
    "worker_compute_min_nodes = 0 \n",
    "worker_compute_max_nodes = 4\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "worker_vm_size = 'Standard_F32s_v2'\n",
    "\n",
    "# Create the compute target if it hasn't been created already\n",
    "if worker_compute_name in ws.compute_targets:\n",
    "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
    "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
    "        if worker_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found worker compute target. just use it', worker_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found worker compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new worker compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=worker_vm_size,\n",
    "        min_nodes=worker_compute_min_nodes,\n",
    "        max_nodes=worker_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the compute target\n",
    "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(worker_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import WorkerConfiguration\n",
    "\n",
    "# Pip packages we will use for both head and worker\n",
    "pip_packages=[\"ray[rllib]==0.8.7\", \"torch\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "# Specify the Ray worker configuration\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure Machine Learning compute target to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=2,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running distributed RL pong training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 66}\\''\n",
    "    \n",
    "\n",
    "}\n",
    "pip_packages_head=[\"ray[rllib]==0.8.7\", \"torch\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    environment = myenv,\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='distributed_rl_from_scratch',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"dqn_pong.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages_head,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=360000,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-13 18:27:29,492 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,738 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,749 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,767 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,839 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,902 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:29,949 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,047 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,073 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,243 WARNING Connection pool is full, discarding connection: management.azure.com\n",
      "2021-01-13 18:27:30,283 WARNING Connection pool is full, discarding connection: management.azure.com\n",
      "2021-01-13 18:27:30,491 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,496 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,726 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,864 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:30,893 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:31,049 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:31,159 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:31,162 WARNING Connection pool is full, discarding connection: management.azure.com\n",
      "2021-01-13 18:27:31,246 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "--- Logging error ---\n",
      "2021-01-13 18:27:32,944 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-13 18:27:33,018 WARNING Connection pool is full, discarding connection: management.azure.com\n",
      "2021-01-13 18:27:44,176 WARNING Connection pool is full, discarding connection: management.azure.com\n"
     ]
    }
   ],
   "source": [
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running distributed RL pong with RLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "training_algorithm = \"IMPALA\"\n",
    "rl_environment = \"PongNoFrameskip-v4\"\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "#     # Training algorithm, IMPALA in this case\n",
    "    \"--run\": training_algorithm,\n",
    "#     \"--use_pytorch\":True,\n",
    "#     \"--checkpoint-freq\": 10,\n",
    "#     \"--checkpoint-at-end\": True,\n",
    "    # Environment, Pong in this case\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Add additional single quotes at the both ends of string values as we have spaces in the \n",
    "    # string parameters, outermost quotes are not passed to scripts as they are not actually part of string\n",
    "    # Number of GPUs\n",
    "    # Number of ray workers\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 36}\\'',\n",
    "        \"--stop\": '\\'{\"episode_reward_mean\": 18, \"time_total_s\": 3600}\\''\n",
    "\n",
    "    # Target episode reward mean to stop the training\n",
    "    # Total training time in seconds\n",
    "#     \"--stop\": '\\'{\"episode_reward_mean\": 0.9, \"time_total_s\": 3600}\\'',\n",
    "}\n",
    "pip_packages_head=[\"ray[rllib]==0.8.7\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator2 = ReinforcementLearningEstimator(\n",
    "#     environment = myenv,\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='distributed_rl_with_rllib/files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"pong_rllib.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=360000,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-10 21:53:55,049 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,235 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,266 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,267 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,440 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,493 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,612 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,659 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,719 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:53:55,766 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:12,942 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:13,105 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:13,158 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:13,171 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:13,205 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:13,212 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n"
     ]
    }
   ],
   "source": [
    "run2 = exp.submit(config=rl_estimator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6025d5d81b3a4756b0640bda56551d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/rl_talk_pong/runs/rl_talk_pong_1610344456_d66a7e69?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent\", \"run_id\": \"rl_talk_pong_1610344456_d66a7e69\", \"run_properties\": {\"run_id\": \"rl_talk_pong_1610344456_d66a7e69\", \"created_utc\": \"2021-01-11T05:54:20.841931Z\", \"properties\": {\"azureml.git.repository_uri\": \"https://github.com/james-tn/distributed-reinforcement-learning.git\", \"mlflow.source.git.repoURL\": \"https://github.com/james-tn/distributed-reinforcement-learning.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"3ec81d3686f3ddd4d8eb7ecbf60901a22b60a6bf\", \"mlflow.source.git.commit\": \"3ec81d3686f3ddd4d8eb7ecbf60901a22b60a6bf\", \"azureml.git.dirty\": \"True\"}, \"tags\": {\"cluster_coordination_timeout_seconds\": \"3600\"}, \"end_time_utc\": \"2021-01-11T06:10:33.093458Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/reinforcementlearning.txt\": \"https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.rl_talk_pong_1610344456_d66a7e69/azureml-logs/reinforcementlearning.txt?sv=2019-02-02&sr=b&sig=2Pyy7o0Sy%2FMUok3xwzEh46b8Tq9fGz7nYG02OuNh3Os%3D&st=2021-01-14T02%3A51%3A15Z&se=2021-01-14T11%3A01%3A15Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/reinforcementlearning.txt\"]], \"run_duration\": \"0:16:12\", \"cluster_coordination_timeout_seconds\": \"3600\"}, \"child_runs\": [{\"run_id\": \"rl_talk_pong_1610344456_d66a7e69_head\", \"run_number\": 137, \"metric\": null, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-01-11T05:54:41.740612Z\", \"end_time\": \"2021-01-11T06:10:25.639817Z\", \"created_time\": \"2021-01-11T05:54:27.400803Z\", \"created_time_dt\": \"2021-01-11T05:54:27.400803Z\", \"duration\": \"0:15:58\"}, {\"run_id\": \"rl_talk_pong_1610344456_d66a7e69_worker\", \"run_number\": 138, \"metric\": null, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-01-11T05:54:39.019205Z\", \"end_time\": \"2021-01-11T06:10:43.735745Z\", \"created_time\": \"2021-01-11T05:54:28.470016Z\", \"created_time_dt\": \"2021-01-11T05:54:28.470016Z\", \"duration\": \"0:16:15\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-01-11T05:54:20.9693784Z][Info]Starting reinforcement learning run with id rl_talk_pong_1610344456_d66a7e69.\\n[2021-01-11T05:54:26.8405921Z][Info]Starting head node child run with id rl_talk_pong_1610344456_d66a7e69_head.\\n[2021-01-11T05:54:27.5075822Z][Info]Starting worker child run with id rl_talk_pong_1610344456_d66a7e69_worker.\\n[2021-01-11T06:10:35.8997294Z][Info]Some child runs have reached terminal state. All active child runs will be cancelled. The run Ids that reached terminal state are: rl_talk_pong_1610344456_d66a7e69_head.\\n[2021-01-11T06:10:35.9454871Z][Info]Updating status of child run with Id rl_talk_pong_1610344456_d66a7e69_worker from Running to Completed, since one of the child runs has reached a terminal state.\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.19.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-10 21:54:38,973 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:38,989 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:38,989 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,042 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,127 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,160 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,206 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,305 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,343 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,376 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:54:39,427 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:05,779 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:05,848 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:05,973 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:06,094 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:06,109 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea42ed416dc4a66975733e81e34f871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-10 21:58:06,135 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n"
     ]
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-10 21:58:06,249 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:06,279 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n",
      "2021-01-10 21:58:06,327 WARNING Connection pool is full, discarding connection: westus2.api.azureml.ms\n"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (generic_research)",
   "language": "python",
   "name": "pycharm-81b73779"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
