{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ray Cluster in AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws01ent | westus2 | azureml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = 'rl_talk_pong'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found head compute target. just use it head-gpu-v2\n"
     ]
    }
   ],
   "source": [
    "vnet_name = 'amlvnet'\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# Choose a name for the Ray head cluster\n",
    "# head_compute_name = 'head-gpu-v3'\n",
    "head_compute_name = 'head-gpu-v2'\n",
    "\n",
    "head_compute_min_nodes = 0\n",
    "head_compute_max_nodes = 2\n",
    "\n",
    "# This example uses GPU VM. For using CPU VM, set SKU to STANDARD_D2_V2\n",
    "head_vm_size = 'STANDARD_NC6S_V2'\n",
    "\n",
    "if head_compute_name in ws.compute_targets:\n",
    "    head_compute_target = ws.compute_targets[head_compute_name]\n",
    "    if head_compute_target and type(head_compute_target) is AmlCompute:\n",
    "        if head_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found head compute target. just use it', head_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found head compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new head compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=head_vm_size,\n",
    "        min_nodes=head_compute_min_nodes, \n",
    "        max_nodes=head_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the cluster\n",
    "    head_compute_target = ComputeTarget.create(ws, head_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    head_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(head_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found worker compute target. just use it worker-cpu-f32\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your Ray worker compute target\n",
    "worker_compute_name = 'worker-cpu-f32'\n",
    "worker_compute_min_nodes = 0 \n",
    "worker_compute_max_nodes = 4\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "worker_vm_size = 'Standard_F32s_v2'\n",
    "\n",
    "# Create the compute target if it hasn't been created already\n",
    "if worker_compute_name in ws.compute_targets:\n",
    "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
    "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
    "        if worker_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found worker compute target. just use it', worker_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found worker compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new worker compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=worker_vm_size,\n",
    "        min_nodes=worker_compute_min_nodes,\n",
    "        max_nodes=worker_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the compute target\n",
    "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(worker_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import WorkerConfiguration\n",
    "\n",
    "# Pip packages we will use for both head and worker\n",
    "pip_packages=[\"ray[rllib]==0.8.7\", \"torch\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "# Specify the Ray worker configuration\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure Machine Learning compute target to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=2,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running distributed RL pong training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 66}\\''\n",
    "    \n",
    "\n",
    "}\n",
    "pip_packages_head=[\"ray[rllib]==0.8.7\", \"torch\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    environment = myenv,\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='distributed_rl_from_scratch',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"dqn_pong.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages_head,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=360000,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042dbd103be54b0bb30bbceeedcf2140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in threading.excepthook:Exception ignored in thread started byException ignored in sys.unraisablehook"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running distributed RL pong with RLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "training_algorithm = \"IMPALA\"\n",
    "rl_environment = \"PongNoFrameskip-v4\"\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "#     # Training algorithm, IMPALA in this case\n",
    "    \"--run\": training_algorithm,\n",
    "#     \"--use_pytorch\":True,\n",
    "#     \"--checkpoint-freq\": 10,\n",
    "#     \"--checkpoint-at-end\": True,\n",
    "    # Environment, Pong in this case\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Add additional single quotes at the both ends of string values as we have spaces in the \n",
    "    # string parameters, outermost quotes are not passed to scripts as they are not actually part of string\n",
    "    # Number of GPUs\n",
    "    # Number of ray workers\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 36}\\'',\n",
    "        \"--stop\": '\\'{\"episode_reward_mean\": 18, \"time_total_s\": 3600}\\''\n",
    "\n",
    "    # Target episode reward mean to stop the training\n",
    "    # Total training time in seconds\n",
    "#     \"--stop\": '\\'{\"episode_reward_mean\": 0.9, \"time_total_s\": 3600}\\'',\n",
    "}\n",
    "pip_packages_head=[\"ray[rllib]==0.8.7\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator2 = ReinforcementLearningEstimator(\n",
    "#     environment = myenv,\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='distributed_rl_with_rllib/files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"pong_rllib.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=360000,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2 = exp.submit(config=rl_estimator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc39bc2f9204b63a5a8fbcceccdfe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in threading.excepthook:Exception ignored in thread started byException ignored in sys.unraisablehook"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run2).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (generic_research)",
   "language": "python",
   "name": "pycharm-81b73779"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
